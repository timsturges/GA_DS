{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " - Predicting discreet categories vs. continuous numbers\n",
    " - ie, boy/girl, green/blue/black vs. stock price, quarterly earnings, etc.\n",
    " - Most techniques work for both regression & classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Linear Models**: Logistic Regression\n",
    "\n",
    "**Random Forests**: Random Forest Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Linear Models and Classification\n",
    "\n",
    " - combine variable coefficients + intercept to arrive at a prediction\n",
    " - only difference is that output is passed through a non-linear activation function called the sigmoid.\n",
    "  - the sigmoid function takes continuous numbers and transforms them into probabilities between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\frac{1}{1 + e^{-x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Your Turn:\n",
    "\n",
    " - Create a function for the sigmoid\n",
    " - you can access the operation for e with the command np.exp()\n",
    " - To test your results:\n",
    "  - sigmoid(0) = 0.5\n",
    "  - sigmoid(1) = 0.73\n",
    "  - sigmoid(2) = 0.88\n",
    "  - sigmoid(-1) = 0.26\n",
    "  - sigmoid(-2) = 0.119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.7310585786300049\n",
      "0.8807970779778823\n",
      "0.2689414213699951\n",
      "0.11920292202211755\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(0))\n",
    "print(sigmoid(1))\n",
    "print(sigmoid(2))\n",
    "print(sigmoid(-1))\n",
    "print(sigmoid(-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case Study:  Who Survived The Titanic?\n",
    "\n",
    "We'll enter the following Kaggle competition to demonstrate how classification works:  https://www.kaggle.com/c/titanic\n",
    "\n",
    "Item to discover:  what chances does someone have of surviving?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('../data/titanic/train.csv')\n",
    "test  = pd.read_csv('../data/titanic/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A Little Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.catplot(x='Sex', y='Survived', kind='bar', data=train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(x='Pclass', y='Survived', kind='bar', data=train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 430.5x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(x='Pclass', y='Survived', kind='bar', hue='Sex', data=train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's Create A Simple Two Variable Model......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train.loc[:, ['Sex', 'Pclass']]\n",
    "y_train = train['Survived']\n",
    "X_test  = test.loc[:, ['Sex', 'Pclass']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X_train, columns=['Sex', 'Pclass'], drop_first=True)\n",
    "X_test  = pd.get_dummies(X_test, columns=['Sex', 'Pclass'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " - How you use linear models for classification\n",
    " - Based off of the sigmoid function\n",
    " - Has arguments for regularization parameter C: the (almost equivalent) as alpha\n",
    " - Also has choice of l1 or l2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'warn',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'liblinear',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " - **C**:  the equivalent of alpha in lasso and ridge regression\n",
    "  - Is inverted.  Ie, value of 0.1 for C equates to 1/10 for regularization parameter\n",
    "  - Highest values signal lowest strength of regularization, and vice versa\n",
    " - **penalty**:  can be either l1, or l2\n",
    "  - ie, you can specify what kind of penalty you want to use\n",
    "  - essentially Ridge and Lasso Regression wrapped up into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can then use the predict method to predic out of sample data\n",
    "logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.20028547, 0.79971453],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.75093089, 0.24906911],\n",
       "       [0.60521334, 0.39478666],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.41462909, 0.58537091],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.11295975, 0.88704025],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527],\n",
       "       [0.89503473, 0.10496527]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also use the predict_proba function to get probabilities for each outcome\n",
    "logreg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10496527, 0.58537091, 0.24906911, 0.10496527, 0.58537091,\n",
       "       0.10496527, 0.58537091, 0.24906911, 0.58537091, 0.10496527,\n",
       "       0.10496527, 0.39478666, 0.88704025, 0.24906911, 0.88704025,\n",
       "       0.79971453, 0.24906911, 0.10496527, 0.58537091, 0.58537091,\n",
       "       0.39478666, 0.10496527, 0.88704025, 0.39478666, 0.88704025,\n",
       "       0.10496527, 0.88704025, 0.10496527, 0.39478666, 0.10496527,\n",
       "       0.24906911, 0.24906911, 0.58537091, 0.58537091, 0.39478666,\n",
       "       0.10496527, 0.58537091, 0.58537091, 0.10496527, 0.10496527,\n",
       "       0.10496527, 0.39478666, 0.10496527, 0.79971453, 0.88704025,\n",
       "       0.10496527, 0.39478666, 0.10496527, 0.88704025, 0.58537091,\n",
       "       0.39478666, 0.24906911, 0.79971453, 0.88704025, 0.24906911,\n",
       "       0.10496527, 0.10496527, 0.10496527, 0.10496527, 0.88704025,\n",
       "       0.10496527, 0.24906911, 0.10496527, 0.58537091, 0.39478666,\n",
       "       0.79971453, 0.58537091, 0.39478666, 0.39478666, 0.88704025,\n",
       "       0.58537091, 0.10496527, 0.58537091, 0.39478666, 0.88704025,\n",
       "       0.39478666, 0.10496527, 0.88704025, 0.24906911, 0.58537091,\n",
       "       0.10496527, 0.39478666, 0.39478666, 0.10496527, 0.24906911,\n",
       "       0.10496527, 0.58537091, 0.58537091, 0.58537091, 0.24906911,\n",
       "       0.58537091, 0.10496527, 0.88704025, 0.10496527, 0.39478666,\n",
       "       0.10496527, 0.88704025, 0.10496527, 0.58537091, 0.10496527,\n",
       "       0.88704025, 0.24906911, 0.10496527, 0.10496527, 0.58537091,\n",
       "       0.10496527, 0.10496527, 0.10496527, 0.10496527, 0.24906911,\n",
       "       0.24906911, 0.58537091, 0.88704025, 0.58537091, 0.88704025,\n",
       "       0.10496527, 0.10496527, 0.58537091, 0.39478666, 0.79971453,\n",
       "       0.79971453, 0.10496527, 0.88704025, 0.10496527, 0.10496527,\n",
       "       0.58537091, 0.10496527, 0.58537091, 0.24906911, 0.10496527,\n",
       "       0.10496527, 0.39478666, 0.58537091, 0.10496527, 0.10496527,\n",
       "       0.10496527, 0.10496527, 0.24906911, 0.58537091, 0.10496527,\n",
       "       0.58537091, 0.88704025, 0.39478666, 0.24906911, 0.39478666,\n",
       "       0.10496527, 0.39478666, 0.10496527, 0.39478666, 0.24906911,\n",
       "       0.88704025, 0.10496527, 0.10496527, 0.58537091, 0.10496527,\n",
       "       0.10496527, 0.88704025, 0.58537091, 0.39478666, 0.58537091,\n",
       "       0.58537091, 0.10496527, 0.79971453, 0.10496527, 0.24906911,\n",
       "       0.58537091, 0.39478666, 0.10496527, 0.88704025, 0.58537091,\n",
       "       0.10496527, 0.10496527, 0.10496527, 0.10496527, 0.10496527,\n",
       "       0.79971453, 0.79971453, 0.39478666, 0.79971453, 0.88704025,\n",
       "       0.24906911, 0.39478666, 0.88704025, 0.10496527, 0.88704025,\n",
       "       0.24906911, 0.79971453, 0.10496527, 0.58537091, 0.24906911,\n",
       "       0.24906911, 0.39478666, 0.10496527, 0.24906911, 0.24906911,\n",
       "       0.10496527, 0.39478666, 0.58537091, 0.24906911, 0.58537091,\n",
       "       0.58537091, 0.10496527, 0.39478666, 0.79971453, 0.24906911,\n",
       "       0.39478666, 0.58537091, 0.24906911, 0.88704025, 0.10496527,\n",
       "       0.10496527, 0.10496527, 0.24906911, 0.79971453, 0.58537091,\n",
       "       0.39478666, 0.58537091, 0.39478666, 0.88704025, 0.10496527,\n",
       "       0.79971453, 0.10496527, 0.79971453, 0.10496527, 0.88704025,\n",
       "       0.58537091, 0.10496527, 0.58537091, 0.10496527, 0.24906911,\n",
       "       0.24906911, 0.88704025, 0.10496527, 0.10496527, 0.39478666,\n",
       "       0.10496527, 0.39478666, 0.10496527, 0.79971453, 0.88704025,\n",
       "       0.88704025, 0.79971453, 0.39478666, 0.10496527, 0.10496527,\n",
       "       0.39478666, 0.79971453, 0.24906911, 0.79971453, 0.58537091,\n",
       "       0.79971453, 0.10496527, 0.39478666, 0.10496527, 0.10496527,\n",
       "       0.10496527, 0.10496527, 0.10496527, 0.79971453, 0.10496527,\n",
       "       0.10496527, 0.10496527, 0.79971453, 0.58537091, 0.24906911,\n",
       "       0.10496527, 0.39478666, 0.10496527, 0.58537091, 0.10496527,\n",
       "       0.39478666, 0.10496527, 0.88704025, 0.58537091, 0.10496527,\n",
       "       0.79971453, 0.24906911, 0.24906911, 0.24906911, 0.24906911,\n",
       "       0.58537091, 0.10496527, 0.58537091, 0.58537091, 0.58537091,\n",
       "       0.10496527, 0.10496527, 0.39478666, 0.10496527, 0.10496527,\n",
       "       0.39478666, 0.58537091, 0.10496527, 0.39478666, 0.10496527,\n",
       "       0.10496527, 0.79971453, 0.10496527, 0.39478666, 0.10496527,\n",
       "       0.10496527, 0.24906911, 0.24906911, 0.10496527, 0.58537091,\n",
       "       0.88704025, 0.39478666, 0.10496527, 0.39478666, 0.58537091,\n",
       "       0.10496527, 0.10496527, 0.10496527, 0.58537091, 0.88704025,\n",
       "       0.58537091, 0.39478666, 0.24906911, 0.10496527, 0.24906911,\n",
       "       0.10496527, 0.10496527, 0.24906911, 0.39478666, 0.88704025,\n",
       "       0.10496527, 0.79971453, 0.39478666, 0.24906911, 0.24906911,\n",
       "       0.79971453, 0.39478666, 0.10496527, 0.58537091, 0.10496527,\n",
       "       0.39478666, 0.24906911, 0.10496527, 0.24906911, 0.10496527,\n",
       "       0.24906911, 0.10496527, 0.10496527, 0.88704025, 0.10496527,\n",
       "       0.58537091, 0.24906911, 0.58537091, 0.24906911, 0.79971453,\n",
       "       0.88704025, 0.24906911, 0.24906911, 0.24906911, 0.58537091,\n",
       "       0.39478666, 0.88704025, 0.10496527, 0.10496527, 0.58537091,\n",
       "       0.10496527, 0.79971453, 0.79971453, 0.10496527, 0.88704025,\n",
       "       0.58537091, 0.10496527, 0.58537091, 0.88704025, 0.24906911,\n",
       "       0.24906911, 0.88704025, 0.39478666, 0.24906911, 0.88704025,\n",
       "       0.88704025, 0.58537091, 0.24906911, 0.39478666, 0.10496527,\n",
       "       0.10496527, 0.10496527, 0.58537091, 0.58537091, 0.24906911,\n",
       "       0.79971453, 0.10496527, 0.24906911, 0.10496527, 0.10496527,\n",
       "       0.39478666, 0.88704025, 0.10496527, 0.24906911, 0.10496527,\n",
       "       0.88704025, 0.10496527, 0.88704025, 0.10496527, 0.10496527,\n",
       "       0.88704025, 0.24906911, 0.88704025, 0.39478666, 0.39478666,\n",
       "       0.24906911, 0.24906911, 0.39478666, 0.58537091, 0.58537091,\n",
       "       0.58537091, 0.88704025, 0.58537091, 0.10496527, 0.88704025,\n",
       "       0.10496527, 0.10496527, 0.10496527])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how you choose the chance that someone will survive\n",
    "logreg.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.06085883])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression is a linear model, so you have coefficients and intercepts\n",
    "logreg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.48809433, -0.67634771, -1.7159975 ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sex_male</td>\n",
       "      <td>-2.488094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pclass_2</td>\n",
       "      <td>-0.676348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pclass_3</td>\n",
       "      <td>-1.715997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Variable    Weight\n",
       "0  Sex_male -2.488094\n",
       "1  Pclass_2 -0.676348\n",
       "2  Pclass_3 -1.715997"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs = pd.DataFrame({\n",
    "    'Variable': X_train.columns,\n",
    "    'Weight'  : logreg.coef_[0]\n",
    "})\n",
    "\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.060859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.344861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.060859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.427235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.344861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.384511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.344861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.060859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.344861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.384511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.103583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.344861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.344861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.103583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.103583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.344861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.427235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.344861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.344861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.427235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.344861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>-1.103583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>2.060859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>0.344861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>-1.103583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>1.384511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>1.384511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>-0.427235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>2.060859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>-0.427235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>1.384511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>0.344861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>2.060859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>1.384511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>0.344861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>-1.103583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>0.344861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>-1.103583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>2.060859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0.344861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>-0.427235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>-2.143233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0   -2.143233\n",
       "1    2.060859\n",
       "2    0.344861\n",
       "3    2.060859\n",
       "4   -2.143233\n",
       "5   -2.143233\n",
       "6   -0.427235\n",
       "7   -2.143233\n",
       "8    0.344861\n",
       "9    1.384511\n",
       "10   0.344861\n",
       "11   2.060859\n",
       "12  -2.143233\n",
       "13  -2.143233\n",
       "14   0.344861\n",
       "15   1.384511\n",
       "16  -2.143233\n",
       "17  -1.103583\n",
       "18   0.344861\n",
       "19   0.344861\n",
       "20  -1.103583\n",
       "21  -1.103583\n",
       "22   0.344861\n",
       "23  -0.427235\n",
       "24   0.344861\n",
       "25   0.344861\n",
       "26  -2.143233\n",
       "27  -0.427235\n",
       "28   0.344861\n",
       "29  -2.143233\n",
       "..        ...\n",
       "861 -1.103583\n",
       "862  2.060859\n",
       "863  0.344861\n",
       "864 -1.103583\n",
       "865  1.384511\n",
       "866  1.384511\n",
       "867 -0.427235\n",
       "868 -2.143233\n",
       "869 -2.143233\n",
       "870 -2.143233\n",
       "871  2.060859\n",
       "872 -0.427235\n",
       "873 -2.143233\n",
       "874  1.384511\n",
       "875  0.344861\n",
       "876 -2.143233\n",
       "877 -2.143233\n",
       "878 -2.143233\n",
       "879  2.060859\n",
       "880  1.384511\n",
       "881 -2.143233\n",
       "882  0.344861\n",
       "883 -1.103583\n",
       "884 -2.143233\n",
       "885  0.344861\n",
       "886 -1.103583\n",
       "887  2.060859\n",
       "888  0.344861\n",
       "889 -0.427235\n",
       "890 -2.143233\n",
       "\n",
       "[891 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's create a variable called output that recreates the predictions\n",
    "output = X_train.dot(logreg.coef_.T) + logreg.intercept_\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.887040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.585371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.887040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.394787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.585371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.799715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.585371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.887040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.585371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.799715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.249069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.585371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.585371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.249069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.249069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.585371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.394787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.585371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.585371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.394787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.585371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>0.249069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>0.887040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>0.585371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>0.249069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>0.799715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>0.799715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>0.394787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>0.887040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>0.394787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>0.799715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>0.585371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>0.887040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>0.799715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>0.585371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>0.249069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>0.585371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0.249069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>0.887040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0.585371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0.394787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0.104965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0    0.104965\n",
       "1    0.887040\n",
       "2    0.585371\n",
       "3    0.887040\n",
       "4    0.104965\n",
       "5    0.104965\n",
       "6    0.394787\n",
       "7    0.104965\n",
       "8    0.585371\n",
       "9    0.799715\n",
       "10   0.585371\n",
       "11   0.887040\n",
       "12   0.104965\n",
       "13   0.104965\n",
       "14   0.585371\n",
       "15   0.799715\n",
       "16   0.104965\n",
       "17   0.249069\n",
       "18   0.585371\n",
       "19   0.585371\n",
       "20   0.249069\n",
       "21   0.249069\n",
       "22   0.585371\n",
       "23   0.394787\n",
       "24   0.585371\n",
       "25   0.585371\n",
       "26   0.104965\n",
       "27   0.394787\n",
       "28   0.585371\n",
       "29   0.104965\n",
       "..        ...\n",
       "861  0.249069\n",
       "862  0.887040\n",
       "863  0.585371\n",
       "864  0.249069\n",
       "865  0.799715\n",
       "866  0.799715\n",
       "867  0.394787\n",
       "868  0.104965\n",
       "869  0.104965\n",
       "870  0.104965\n",
       "871  0.887040\n",
       "872  0.394787\n",
       "873  0.104965\n",
       "874  0.799715\n",
       "875  0.585371\n",
       "876  0.104965\n",
       "877  0.104965\n",
       "878  0.104965\n",
       "879  0.887040\n",
       "880  0.799715\n",
       "881  0.104965\n",
       "882  0.585371\n",
       "883  0.249069\n",
       "884  0.104965\n",
       "885  0.585371\n",
       "886  0.249069\n",
       "887  0.887040\n",
       "888  0.585371\n",
       "889  0.394787\n",
       "890  0.104965\n",
       "\n",
       "[891 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and this gives us our predictions\n",
    "sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# now let's submit predictions\n",
    "preds = logreg.predict(X_test)\n",
    "\n",
    "lm_predictions = pd.DataFrame({\n",
    "    'PassengerId': test.PassengerId,\n",
    "    'Survived'   : preds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lm_predictions.to_csv('../data/titanic/lm_submissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forest Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " - Work in a manner that's almost identical to their Regressor Counterpart\n",
    " - Classify a sample by taking a majority vote of each leaf value\n",
    " - Basic syntax is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we'll fit a quick model\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71278892, 0.03139943, 0.25581165])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sex_male</td>\n",
       "      <td>0.712789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pclass_2</td>\n",
       "      <td>0.031399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pclass_3</td>\n",
       "      <td>0.255812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Features  Importance\n",
       "0  Sex_male    0.712789\n",
       "1  Pclass_2    0.031399\n",
       "2  Pclass_3    0.255812"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = pd.DataFrame({\n",
    "    'Features': X_train.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "})\n",
    "\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.08097277, 0.91902723],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.84083881, 0.15916119],\n",
       "       [0.62553242, 0.37446758],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.51834828, 0.48165172],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.02762347, 0.97237653],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786],\n",
       "       [0.86607214, 0.13392786]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# make the predictions\n",
    "rf_preds = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# pass them into a dataframe\n",
    "rf_submissions = pd.DataFrame({\n",
    "    'PassengerId': test.PassengerId,\n",
    "    'Survived'   : rf_preds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# make random forest submissions\n",
    "rf_submissions.to_csv('../data/titanic/rf_submissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A very useful way to make comparisons between different models.\n",
    "\n",
    "Sets up a graph of options to try out, according to different parameters that you specify.  \n",
    "\n",
    "Produces useful output for model results that are useful for analysis.\n",
    "\n",
    "Can incorporate cross-validation into your model tuning automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Basically happens in 3 different steps: \n",
    "\n",
    " - define parameters that you want to test\n",
    " - load them into a grid\n",
    " - fit (this can take a while!)\n",
    " - look at ~best_params~ and ~cv_results_~ attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# set up the parameters of the model you'd like to fit\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C'      : [.0001, .001, .01, .1, 1, 10, 100, 1000, 10000],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load it into the grid\n",
    "grid = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit it on your training data\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the version that gave you the best fit\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# and best of all, the results of all your cross-validation\n",
    "grid_results = pd.DataFrame(grid.cv_results_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003145</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.006212</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 0.0001, 'penalty': 'l1'}</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.616438</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004588</td>\n",
       "      <td>0.006744</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.004704</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 0.0001, 'penalty': 'l2'}</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.616438</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003545</td>\n",
       "      <td>0.006707</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 0.001, 'penalty': 'l1'}</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.616438</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002671</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.004780</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 0.001, 'penalty': 'l2'}</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.616438</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.003584</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 0.01, 'penalty': 'l1'}</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.61596</td>\n",
       "      <td>0.616438</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.003145      0.004862         0.003693        0.006212  0.0001   \n",
       "1       0.004588      0.006744         0.001813        0.004704  0.0001   \n",
       "2       0.003545      0.006707         0.000929        0.002788   0.001   \n",
       "3       0.002671      0.003342         0.001958        0.004780   0.001   \n",
       "4       0.003609      0.003584         0.001252        0.002438    0.01   \n",
       "\n",
       "  param_penalty                          params  split0_test_score  \\\n",
       "0            l1  {'C': 0.0001, 'penalty': 'l1'}           0.611111   \n",
       "1            l2  {'C': 0.0001, 'penalty': 'l2'}           0.611111   \n",
       "2            l1   {'C': 0.001, 'penalty': 'l1'}           0.611111   \n",
       "3            l2   {'C': 0.001, 'penalty': 'l2'}           0.611111   \n",
       "4            l1    {'C': 0.01, 'penalty': 'l1'}           0.611111   \n",
       "\n",
       "   split1_test_score  split2_test_score       ...         split2_train_score  \\\n",
       "0           0.611111           0.617978       ...                    0.61596   \n",
       "1           0.611111           0.617978       ...                    0.61596   \n",
       "2           0.611111           0.617978       ...                    0.61596   \n",
       "3           0.611111           0.617978       ...                    0.61596   \n",
       "4           0.611111           0.617978       ...                    0.61596   \n",
       "\n",
       "   split3_train_score  split4_train_score  split5_train_score  \\\n",
       "0             0.61596             0.61596             0.61596   \n",
       "1             0.61596             0.61596             0.61596   \n",
       "2             0.61596             0.61596             0.61596   \n",
       "3             0.61596             0.61596             0.61596   \n",
       "4             0.61596             0.61596             0.61596   \n",
       "\n",
       "   split6_train_score  split7_train_score  split8_train_score  \\\n",
       "0             0.61596             0.61596             0.61596   \n",
       "1             0.61596             0.61596             0.61596   \n",
       "2             0.61596             0.61596             0.61596   \n",
       "3             0.61596             0.61596             0.61596   \n",
       "4             0.61596             0.61596             0.61596   \n",
       "\n",
       "   split9_train_score  mean_train_score  std_train_score  \n",
       "0            0.616438          0.616162         0.000317  \n",
       "1            0.616438          0.616162         0.000317  \n",
       "2            0.616438          0.616162         0.000317  \n",
       "3            0.616438          0.616162         0.000317  \n",
       "4            0.616438          0.616162         0.000317  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and here are the results from each round\n",
    "grid_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n",
       "       'param_C', 'param_penalty', 'params', 'split0_test_score',\n",
       "       'split1_test_score', 'split2_test_score', 'split3_test_score',\n",
       "       'split4_test_score', 'split5_test_score', 'split6_test_score',\n",
       "       'split7_test_score', 'split8_test_score', 'split9_test_score',\n",
       "       'mean_test_score', 'std_test_score', 'rank_test_score',\n",
       "       'split0_train_score', 'split1_train_score', 'split2_train_score',\n",
       "       'split3_train_score', 'split4_train_score', 'split5_train_score',\n",
       "       'split6_train_score', 'split7_train_score', 'split8_train_score',\n",
       "       'split9_train_score', 'mean_train_score', 'std_train_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# every column returned by our new dataframe\n",
    "grid_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's trim this up a bit\n",
    "cols = ['param_C', 'param_penalty', 'mean_test_score', 'std_test_score', 'rank_test_score', 'mean_train_score', 'std_train_score']\n",
    "grid_results = grid_results.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.027926</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.003101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.027926</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.003101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.027926</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.003101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.027926</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.003101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.027926</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.003101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.027926</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.003101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.027926</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.003101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.027926</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.003101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10000</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.027926</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.003101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.027926</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.003101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.027926</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.003101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.033685</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786758</td>\n",
       "      <td>0.003748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10000</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.027926</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.003101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>14</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>14</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>14</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>14</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>14</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_C param_penalty  mean_test_score  std_test_score  rank_test_score  \\\n",
       "8        1            l1         0.786756        0.027926                1   \n",
       "15    1000            l2         0.786756        0.027926                1   \n",
       "14    1000            l1         0.786756        0.027926                1   \n",
       "13     100            l2         0.786756        0.027926                1   \n",
       "12     100            l1         0.786756        0.027926                1   \n",
       "11      10            l2         0.786756        0.027926                1   \n",
       "10      10            l1         0.786756        0.027926                1   \n",
       "9        1            l2         0.786756        0.027926                1   \n",
       "17   10000            l2         0.786756        0.027926                1   \n",
       "7      0.1            l2         0.786756        0.027926                1   \n",
       "6      0.1            l1         0.786756        0.027926                1   \n",
       "5     0.01            l2         0.786756        0.033685                1   \n",
       "16   10000            l1         0.786756        0.027926                1   \n",
       "4     0.01            l1         0.616162        0.002844               14   \n",
       "3    0.001            l2         0.616162        0.002844               14   \n",
       "2    0.001            l1         0.616162        0.002844               14   \n",
       "1   0.0001            l2         0.616162        0.002844               14   \n",
       "0   0.0001            l1         0.616162        0.002844               14   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "8           0.786756         0.003101  \n",
       "15          0.786756         0.003101  \n",
       "14          0.786756         0.003101  \n",
       "13          0.786756         0.003101  \n",
       "12          0.786756         0.003101  \n",
       "11          0.786756         0.003101  \n",
       "10          0.786756         0.003101  \n",
       "9           0.786756         0.003101  \n",
       "17          0.786756         0.003101  \n",
       "7           0.786756         0.003101  \n",
       "6           0.786756         0.003101  \n",
       "5           0.786758         0.003748  \n",
       "16          0.786756         0.003101  \n",
       "4           0.616162         0.000317  \n",
       "3           0.616162         0.000317  \n",
       "2           0.616162         0.000317  \n",
       "1           0.616162         0.000317  \n",
       "0           0.616162         0.000317  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results.sort_values(by='rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Useful Things To Do With Your Grid Results**:\n",
    "\n",
    " - groupby operations\n",
    " - run regressions on a target variable, like mean_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Your Turn:\n",
    "\n",
    "Create your own grid search for a random forest, and create one that tests the following parameters:\n",
    "\n",
    " - min_sample_leaf\n",
    " - n_estimators\n",
    " - max_features\n",
    " \n",
    "**Try and use a more complicated dataset than the one used here, to make your results more interesting!**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
