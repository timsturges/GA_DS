{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prompt:\n",
    "\n",
    "What are your working definitions of Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Useful Answer:  \n",
    "\n",
    "A subset of Machine Learning That Uses Multiple Layers of Computation to Extract Abstract Features From Raw Input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More Conversational Answer:\n",
    "\n",
    "Doing fun things with Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### What Is Deep Learning Meant To Do?\n",
    "\n",
    " - Work best with raw, unstructured data\n",
    " - Best at finding subtle, non-linear patterns in data that isn't clearly marked for human consumption\n",
    " - Can be very computationally expensive -- fitting times are often days/weeks long for final work\n",
    " - Primarily run on GPU's\n",
    " - Use a different set of libraries to fit -- Tensorflow or PyTorch\n",
    " - Mostly used in classification for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### So How Does Deep Learning Work?  \n",
    "\n",
    " 1.  Take in raw data\n",
    " 2.  Randomly initialize sets of weights for every single neuron within the network\n",
    " 3.  Create linear combination between your data and your computation layer\n",
    " 4.  Take the output from step 3, and pass it through a *non-linear activation function*\n",
    " 5.  Take this data, and pass it through to your next layer, repeat steps 3 & 4\n",
    " 6.  Proceed until there are no more units left, at which point you make your prediction\n",
    " 7.  Use the error term in your prediction to update the values of your weights\n",
    " 8.  Repeat steps 1-7 all over again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural Network Layers\n",
    "\n",
    " - Layers are the basic unit of neural network construction\n",
    " - Consist of 3 basic types:\n",
    "  - **Input Layer**: Your dataset\n",
    "  - **Output Layer**: Your set of probabilities for your prediction (ie, sklearn's predict_proba method)\n",
    "  - **Hidden Layer**: Intermediate layers of computation that are used to detect non-linear patterns within your data\n",
    "  \n",
    "**Somewhat Interesting Note**:  A neural network without a hidden layer is just a linear model!  You can think of Logistic Regression as the simplest possible implementation of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case Study: A Simple 3 Layer Neural Network That Predicts A Binary Outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1:  Creating the Input Layer\n",
    "\n",
    " - **Clue:**  THIS IS ALWAYS YOUR DATASET!\n",
    " - If it is multidimensional, then you'll have to flatten it before feeding it to something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's create some synthetic data\n",
    "X = np.random.normal(loc=0.0, scale=.1, size=10000).reshape(1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have a data set with 10000 rows + 10 columns -- our input layer\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Hidden Layer(s) \n",
    "\n",
    " - Basically what makes a neural network a neural network\n",
    " - A neural network with at least 2 hidden layers is 'Deep Learning'\n",
    " - Are what is needed to create the subtle patterns of non-linearity in your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's pretend that we have a linear model\n",
    "coef_      = np.random.random(10)\n",
    "intercept_ = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.14117135e-01,  1.89116614e-01,  2.54843852e-01,  2.73971715e-01,\n",
       "        3.63089909e-01,  2.01218408e-01,  3.81355101e-01,  2.23998936e-01,\n",
       "        2.68179686e-01,  3.41797375e-01,  9.80392422e-02,  2.27624378e-01,\n",
       "        3.60333655e-01,  4.14284310e-01,  7.79933528e-02,  1.20234819e-02,\n",
       "        1.66830153e-01,  1.63099871e-01,  1.51837473e-01,  3.24021886e-01,\n",
       "        1.37006318e-01,  6.75806462e-02,  3.88265067e-01,  1.29081324e-01,\n",
       "        2.08381632e-01,  1.48597799e-01,  2.15641003e-01,  1.47088298e-01,\n",
       "        2.62606244e-01,  1.17455772e-01, -3.58945318e-04,  3.11797990e-01,\n",
       "        3.66925873e-01,  2.11866187e-01,  1.10630074e-02,  1.12665875e-01,\n",
       "        1.76394002e-02,  1.24263567e-01,  8.62156904e-02,  3.09995493e-01,\n",
       "        3.57731275e-01,  2.45099897e-01,  4.66268447e-01,  3.69762324e-02,\n",
       "        1.87907902e-01,  2.61121351e-01,  1.16979201e-01,  1.93576857e-01,\n",
       "        3.18096881e-01,  3.04810542e-01,  2.20135477e-01,  1.51586083e-01,\n",
       "        1.48314209e-01,  1.95448050e-01,  2.20506513e-01,  1.86985531e-01,\n",
       "        2.83878586e-01,  2.25380975e-01,  2.39931862e-01,  2.04663973e-01,\n",
       "        1.99755760e-01,  1.30080130e-01,  2.63575256e-01,  3.08978961e-01,\n",
       "        1.00900859e-01,  2.40186698e-01,  2.52496906e-01,  3.03419594e-01,\n",
       "        8.72394173e-02,  1.75553686e-01,  1.15247846e-01,  6.75407314e-02,\n",
       "        2.02715221e-01,  8.30554962e-02,  2.31003461e-02,  3.13404934e-01,\n",
       "        9.66452086e-02,  9.81624038e-02,  1.89221587e-01,  2.76387149e-01,\n",
       "        9.84803111e-02,  1.26079166e-01,  2.06253019e-01,  2.07510309e-02,\n",
       "        6.79937179e-02,  1.92126429e-01,  3.56828857e-01,  3.53679378e-01,\n",
       "        1.35058413e-01,  2.97829873e-01,  4.65719717e-01,  2.51247316e-01,\n",
       "        1.05152681e-01,  1.78894426e-01,  3.01445818e-01,  3.68033277e-01,\n",
       "        1.92198038e-01,  9.43664087e-02,  2.40241973e-01,  1.20657331e-01,\n",
       "        2.60881487e-01, -2.41034378e-02,  2.01962125e-01,  3.59063911e-01,\n",
       "        2.82615451e-01,  2.84075215e-01,  4.07193508e-01,  2.17173299e-01,\n",
       "        1.77357376e-01,  2.59983393e-01,  2.28302853e-01,  1.65287853e-01,\n",
       "        3.14560106e-01,  1.38844363e-01,  4.58844713e-02,  1.42294075e-01,\n",
       "        2.51047406e-01,  4.96183274e-02,  3.39518098e-02,  1.56870150e-01,\n",
       "        1.70308977e-01,  1.79720653e-01,  3.24579339e-01,  1.94912854e-01,\n",
       "        4.49631239e-01,  1.95346210e-01,  2.44324086e-01,  1.11027850e-01,\n",
       "        1.22745263e-01,  2.44021569e-01,  2.28417430e-01,  1.99991066e-01,\n",
       "        4.13666960e-01,  3.04692170e-01,  1.85659259e-01,  1.04889893e-01,\n",
       "        2.57211255e-01,  1.87654629e-01,  1.50599600e-01,  2.58858253e-01,\n",
       "       -3.26768935e-02,  3.07925834e-01,  1.34085130e-01,  2.06401055e-01,\n",
       "        2.42488983e-01,  2.80601655e-01,  1.87252933e-01,  8.28155686e-02,\n",
       "        2.33952883e-01,  2.62383229e-01,  3.37242215e-01,  2.05093121e-01,\n",
       "        3.58107709e-01,  1.52904721e-01,  2.56154538e-01,  3.25961230e-02,\n",
       "        1.95979590e-02, -3.16979830e-03,  4.11632108e-01,  4.56711040e-02,\n",
       "        1.92435788e-01,  6.65491535e-02,  1.76211865e-01,  2.75706346e-01,\n",
       "        1.47788057e-01,  2.22784125e-01,  4.50147258e-02,  1.72473337e-01,\n",
       "        1.54087855e-01,  1.17930022e-01,  1.34168176e-01,  1.29259089e-01,\n",
       "        3.02265360e-01,  3.18701673e-01,  2.12987450e-01,  7.42689032e-03,\n",
       "        2.42533417e-01,  1.33761414e-01,  1.11578892e-01,  1.26536290e-01,\n",
       "        4.89940450e-01, -1.04640401e-01,  1.71131602e-01,  3.58213182e-01,\n",
       "        6.15699088e-02,  3.18154425e-01,  2.12985415e-01,  1.26346163e-01,\n",
       "        2.39752085e-01,  2.76513496e-01,  2.66452775e-01,  3.77735434e-01,\n",
       "        9.95865923e-02,  8.55767637e-02,  2.24049939e-01,  9.69790229e-02,\n",
       "        1.81992518e-01,  2.98086272e-04,  3.96038793e-01,  2.77852426e-01,\n",
       "        6.76537156e-02,  9.04241977e-02,  1.34239173e-01,  2.11033488e-01,\n",
       "        2.76049684e-01,  2.43664046e-01,  2.80911533e-01,  3.57672966e-01,\n",
       "       -5.64506562e-02,  1.16528119e-01,  2.44369396e-01,  1.80824979e-01,\n",
       "        2.03064207e-01,  3.94760121e-01, -1.28349743e-02,  2.33043563e-01,\n",
       "        3.08434411e-01,  2.29509015e-01, -2.01080523e-01,  2.49928975e-02,\n",
       "        1.92204520e-01,  2.77849868e-01,  1.42039212e-01,  3.18929441e-01,\n",
       "        2.14782673e-01,  3.19840580e-01,  2.31099175e-01,  3.34607339e-01,\n",
       "        2.88462922e-01,  3.44603520e-01,  3.12873488e-01,  2.16190526e-01,\n",
       "       -3.06927267e-02,  5.24583310e-01,  9.35392785e-02,  9.81325312e-03,\n",
       "        1.91843736e-01,  3.13389029e-01, -1.05937445e-01,  1.05764335e-01,\n",
       "       -4.45242230e-03,  2.67916797e-01,  2.70733875e-01,  1.28003287e-01,\n",
       "        1.48693848e-01,  2.07053554e-01,  2.72618935e-01,  2.08044975e-01,\n",
       "       -3.23452555e-02,  7.95201490e-02,  3.08426860e-01,  2.06394122e-01,\n",
       "        4.01216158e-02,  2.31311837e-01,  1.53709097e-02,  2.88329595e-01,\n",
       "        1.37621015e-01,  9.43270167e-02,  9.16900996e-02,  1.16399045e-01,\n",
       "        2.09147204e-01,  4.27959681e-01,  2.62193762e-01,  4.00950401e-01,\n",
       "        1.40373099e-01,  3.23374373e-01, -5.37143305e-02,  1.64846470e-01,\n",
       "        1.08669216e-01,  2.33899638e-01,  2.03792422e-01,  3.55681350e-01,\n",
       "        1.18097721e-01,  3.33650672e-01,  1.10036907e-01, -1.86196133e-02,\n",
       "        3.38118447e-01,  1.81838164e-01,  2.85167651e-01,  2.53520101e-01,\n",
       "        4.33497511e-01,  1.69236333e-01,  7.36371990e-02,  1.73765045e-01,\n",
       "       -2.04498951e-02,  3.04729626e-01,  2.40679470e-01,  1.23182943e-01,\n",
       "        3.93827449e-01, -3.87642779e-02,  3.48286625e-01, -2.44299241e-02,\n",
       "        4.05942904e-01,  1.76648455e-01,  1.31233904e-01,  1.16292742e-01,\n",
       "        1.30781106e-01,  1.74550652e-01,  1.12275074e-01,  2.91010973e-01,\n",
       "        4.12337800e-01,  1.84731407e-01,  7.01094098e-02,  1.57494804e-01,\n",
       "        1.99996651e-01,  1.62060138e-01,  1.50393867e-01,  9.51432872e-03,\n",
       "        2.70964871e-01,  3.39825306e-01,  1.09874012e-01,  1.96179623e-01,\n",
       "        1.29228522e-01,  2.55903772e-01,  2.09608417e-01,  2.65344138e-01,\n",
       "        1.75436142e-01, -4.90168578e-02,  1.86296974e-01,  2.56074867e-01,\n",
       "        8.64143833e-02,  2.95066857e-01,  1.57266124e-01,  3.85150450e-01,\n",
       "        2.50215577e-01,  5.76545805e-02,  5.59268176e-02,  1.53366001e-01,\n",
       "       -2.59802990e-02,  2.17182961e-01, -3.16621319e-02,  2.96502280e-01,\n",
       "        3.24531104e-01,  2.55003590e-01,  4.66235777e-02,  1.56202867e-01,\n",
       "        2.29093477e-01,  2.13036657e-01,  3.12333192e-01,  4.12506445e-01,\n",
       "        1.44337422e-02,  2.96207266e-01,  2.40512170e-01,  2.97927542e-01,\n",
       "        2.59869289e-01,  3.74599415e-01,  1.99631825e-01,  3.37446727e-01,\n",
       "        3.68415507e-01,  3.11812503e-01,  2.67220587e-01,  2.99936337e-01,\n",
       "        2.83164280e-01,  1.98366002e-01,  1.03321869e-01,  4.42259357e-01,\n",
       "        2.01038795e-01,  1.16474057e-01,  2.02175828e-01,  9.51259468e-02,\n",
       "        2.51928690e-01,  1.61068479e-01,  1.97270607e-01,  2.97195953e-01,\n",
       "        7.53815241e-02,  3.37560479e-01,  2.25679192e-01,  1.00540093e-01,\n",
       "        3.15901608e-01,  2.04987753e-01,  2.59413502e-01,  2.57490651e-01,\n",
       "        3.15483300e-01,  2.87864100e-01,  4.04756218e-01,  2.09762641e-01,\n",
       "        2.20197425e-01,  1.91472486e-01,  1.31839090e-01,  1.86913204e-01,\n",
       "        2.27673102e-02,  2.72571128e-01,  1.07289049e-01,  2.17837613e-01,\n",
       "        2.85558931e-01,  2.35574802e-01,  1.91099122e-01,  3.24591620e-01,\n",
       "        2.19332197e-01,  2.56938471e-01,  1.30499990e-01,  1.55932090e-01,\n",
       "        2.26020020e-01,  2.84661875e-01,  1.84625730e-01,  1.95486542e-01,\n",
       "       -3.96624220e-02,  2.90975381e-01,  2.46042792e-01,  1.31718959e-01,\n",
       "        2.30492304e-01,  2.79036920e-01,  3.38034586e-02,  2.60579412e-01,\n",
       "        3.68058777e-01,  2.87772234e-01,  1.88698607e-01,  9.62029538e-02,\n",
       "        1.20764911e-01,  2.91305299e-01,  4.52527345e-02,  1.27233403e-01,\n",
       "        2.09452277e-01,  1.48828830e-01,  1.21873234e-01,  2.92559625e-01,\n",
       "        2.36765944e-01,  2.37461468e-01,  1.48038595e-01,  1.84694303e-01,\n",
       "        2.32514015e-01,  9.05489359e-02,  1.66277832e-01,  3.82573120e-01,\n",
       "        1.68377794e-01,  4.66846612e-01,  1.51182321e-01,  1.07989813e-01,\n",
       "        3.70207654e-03,  1.52409051e-01,  2.36544641e-01,  2.27227571e-01,\n",
       "        2.50764331e-01,  2.02401935e-01,  2.13481727e-01,  1.08377699e-01,\n",
       "        3.76755969e-01,  1.28909940e-01, -1.38430005e-02,  8.22497677e-02,\n",
       "        1.10015566e-01,  8.68374087e-02,  2.71882866e-01,  8.86395049e-02,\n",
       "        3.95099472e-01,  2.59967326e-01,  2.69238147e-01,  1.46578653e-01,\n",
       "        1.69705351e-01,  2.41632235e-01,  1.82283951e-01,  1.40974993e-01,\n",
       "        1.23616637e-01,  1.43750071e-01,  2.55072983e-01,  2.89076079e-01,\n",
       "        3.88299525e-01,  1.02215603e-01,  3.16875258e-01,  2.30493271e-01,\n",
       "        9.86017822e-02,  3.10205339e-01,  7.06198626e-02,  2.14732575e-01,\n",
       "        1.81742300e-01,  5.67060688e-03,  1.62219829e-01,  1.53520638e-01,\n",
       "        1.55111776e-01,  1.79330800e-01, -9.76846308e-02,  7.70093922e-02,\n",
       "        1.32101106e-01,  1.30309224e-01,  2.69012772e-01,  2.55533123e-01,\n",
       "        8.50195743e-02,  2.21906433e-01,  1.11324487e-01,  1.63497930e-01,\n",
       "        8.76449433e-02,  1.92914770e-01,  1.17938759e-01,  2.61255408e-01,\n",
       "        2.95449402e-01,  1.89438109e-01,  2.93199533e-01,  3.34806521e-01,\n",
       "        3.40067895e-01,  2.49960990e-01,  4.74498717e-02,  1.41115280e-01,\n",
       "        3.85924704e-01,  3.15318376e-01,  1.05855702e-01,  2.54358881e-01,\n",
       "        3.03454077e-01,  2.98553110e-01,  2.74317756e-01,  2.94647371e-01,\n",
       "        2.17916814e-01,  1.26358976e-01,  2.52650951e-01,  2.23721907e-01,\n",
       "        3.81876250e-01,  5.26366908e-01, -7.96584142e-02,  2.99859106e-01,\n",
       "        1.57380353e-01,  1.77543503e-01,  5.62197005e-02,  2.47867856e-01,\n",
       "        3.62307701e-01,  4.93618234e-02,  1.08817368e-01,  3.29992434e-01,\n",
       "        1.99613970e-01,  1.32720701e-01,  3.06349995e-01,  6.45833386e-02,\n",
       "        2.59712258e-01,  1.03232782e-01,  1.70864502e-01,  3.02178118e-01,\n",
       "        4.18948306e-01,  9.08752075e-02, -2.12180712e-02,  3.35253305e-01,\n",
       "        9.25124579e-02,  5.89901359e-02,  2.45301880e-01,  7.84124236e-02,\n",
       "        9.97767103e-02,  2.34003689e-01,  1.90230942e-01,  5.51039995e-02,\n",
       "        3.08789696e-01,  2.05064546e-01,  7.64815504e-02,  2.14919971e-01,\n",
       "        1.98001609e-01,  1.30816791e-01,  3.04016084e-01,  1.69403305e-01,\n",
       "        2.56959058e-01,  1.53917163e-01,  4.07830012e-01,  2.90633637e-01,\n",
       "        8.47628579e-02,  1.58300947e-01,  2.43747205e-01,  2.92119545e-01,\n",
       "        2.52105128e-01,  2.25427934e-01,  5.48894010e-02,  9.82916959e-02,\n",
       "        3.23095795e-01,  9.67767345e-02,  4.77098252e-01,  2.06760914e-01,\n",
       "        4.65348033e-01,  4.65236245e-02,  4.68327133e-02,  8.58026278e-02,\n",
       "        1.70659223e-01,  4.60253549e-02,  1.31879782e-01,  1.49245094e-01,\n",
       "        2.33358731e-01,  2.48221929e-01,  6.08685999e-02,  4.22209688e-01,\n",
       "        1.59276868e-01,  5.13298741e-02,  4.39348795e-01,  1.54296242e-01,\n",
       "        1.60852178e-01,  1.79931348e-01,  1.29297815e-01,  2.73893231e-01,\n",
       "        5.82995253e-02,  1.30559173e-01,  3.59586704e-01, -1.37136461e-02,\n",
       "        2.06707070e-01,  2.94803839e-01,  3.26806050e-01,  1.99314208e-01,\n",
       "        1.28922353e-01,  2.82650755e-01,  3.51950032e-01,  2.10567260e-01,\n",
       "        1.31263437e-01,  1.90775897e-01, -3.30766269e-02,  2.17918168e-01,\n",
       "        1.82307988e-01,  2.20551308e-01,  3.80427226e-01,  3.58139755e-01,\n",
       "        2.97948500e-01,  4.17254011e-01,  3.89096889e-01,  1.17256961e-01,\n",
       "        2.18486734e-01,  6.87535971e-02,  3.37471313e-01,  2.21124278e-01,\n",
       "        2.29179336e-01,  3.33079397e-01,  2.76506297e-01,  2.32162145e-01,\n",
       "        2.03982184e-01,  1.96751847e-01,  1.80586685e-01,  1.50636111e-01,\n",
       "        2.80624448e-01,  4.34401935e-01,  1.98515560e-01,  1.50796844e-01,\n",
       "        1.83701841e-01,  1.71925390e-01,  1.05162687e-02,  9.48326634e-02,\n",
       "        1.01183607e-01,  2.41902942e-01,  2.03548241e-01,  2.01333161e-01,\n",
       "       -2.57005788e-04,  3.20718639e-01,  1.80921895e-01,  1.88659616e-01,\n",
       "        1.25529868e-01,  2.12996681e-01,  1.09500445e-01,  2.73223697e-01,\n",
       "        3.99562009e-01,  1.39861135e-01,  7.07552462e-02, -8.92726542e-03,\n",
       "        2.37363174e-01,  1.51981893e-01,  3.77651065e-01,  1.02724889e-01,\n",
       "        2.52623497e-01,  3.71608526e-01,  4.17589094e-01,  3.23425905e-02,\n",
       "        2.48513101e-01,  7.34444502e-02,  2.88788651e-01,  2.67825611e-01,\n",
       "        2.38010244e-01,  2.48088126e-01,  1.47146140e-01,  2.82866674e-01,\n",
       "       -8.00315452e-02,  2.06301825e-01,  4.61567456e-02,  1.24256099e-01,\n",
       "        4.86022797e-01,  3.41666358e-01,  1.65855616e-01,  1.76193902e-01,\n",
       "        4.06751591e-01,  1.10833162e-01,  2.50498557e-01,  1.97379739e-01,\n",
       "        3.48950007e-01,  1.55973147e-01,  4.32814658e-01,  1.46658969e-01,\n",
       "        2.71774001e-01, -6.73685020e-03,  2.56427506e-01,  9.59583832e-02,\n",
       "        2.37614841e-01,  2.05035939e-01,  6.63399041e-02,  1.10317741e-01,\n",
       "        8.24091311e-02,  1.69532016e-01,  3.46391674e-01, -4.30466757e-03,\n",
       "        2.68278962e-01,  3.86415623e-01,  5.52346557e-01,  2.45133459e-01,\n",
       "        1.88570950e-01,  2.95254686e-01,  2.96704791e-01,  2.59372965e-01,\n",
       "        2.54636542e-01,  3.12685248e-01,  3.01746385e-01,  2.13355507e-01,\n",
       "        1.05324689e-01,  2.61730059e-01,  1.34125879e-01,  3.81750367e-03,\n",
       "        1.69729619e-01,  3.22148809e-01,  1.96308748e-01,  2.42661988e-01,\n",
       "        1.56493431e-02,  8.14379575e-02,  3.63144048e-01,  2.23868460e-01,\n",
       "        1.26338380e-02,  6.46664038e-02,  1.77583434e-01,  2.27025907e-01,\n",
       "        7.72573282e-02,  1.28972532e-01,  3.69361931e-01,  1.64049704e-01,\n",
       "        3.34665876e-01,  1.68080010e-01, -2.11968220e-02,  4.11741103e-01,\n",
       "        4.00650961e-01,  2.45028525e-01,  3.84748639e-01,  1.26457690e-01,\n",
       "       -5.74124708e-02,  4.11249374e-01,  7.70804276e-02,  8.15728432e-02,\n",
       "        2.83876294e-01,  2.28640573e-01,  3.20862805e-01,  2.90747856e-01,\n",
       "        2.69341519e-01,  1.90160250e-01,  2.29767768e-01, -3.09370068e-03,\n",
       "        2.67056347e-01,  5.83602424e-02,  2.74706606e-01,  3.64607776e-01,\n",
       "        1.25482711e-01,  7.62612595e-02,  3.75326182e-01,  2.37394231e-01,\n",
       "        2.50786913e-01,  3.74464054e-01,  3.78737076e-01,  3.31996310e-01,\n",
       "        2.45310526e-01,  1.09258381e-01,  2.88790666e-01,  9.47899876e-02,\n",
       "        2.79725132e-01,  3.30993180e-01,  1.84403945e-01,  2.07489136e-01,\n",
       "        1.45643090e-01,  2.65112301e-01,  2.49773414e-01,  2.16829357e-01,\n",
       "        2.48705207e-01,  2.17183000e-01,  1.81216405e-01,  1.81715496e-01,\n",
       "        1.89381313e-01,  1.29987558e-01,  1.01605562e-01,  2.73114656e-01,\n",
       "        2.68053214e-01,  8.37529493e-02,  4.95066044e-02,  9.54844714e-02,\n",
       "        1.52594920e-01,  3.35344824e-01,  2.87618865e-01,  2.69487843e-01,\n",
       "        2.23596042e-01,  4.10783753e-01,  1.93656576e-01,  9.22833957e-02,\n",
       "        6.99509996e-02,  8.54519851e-02,  1.69756110e-01,  1.94855937e-01,\n",
       "        8.70970678e-03,  3.20493210e-01,  5.49531565e-02,  2.38054843e-01,\n",
       "        2.44913099e-01,  2.71270686e-01,  1.41975252e-01,  3.19593843e-01,\n",
       "        2.92888637e-01,  1.60050517e-01,  3.23586705e-01,  2.54762310e-01,\n",
       "        2.12882536e-01,  1.12425799e-01,  2.28808378e-01,  1.47318375e-01,\n",
       "        6.81002652e-02,  1.82442557e-01,  2.42153234e-01,  3.52768260e-01,\n",
       "        1.86860616e-01,  1.66428950e-01,  4.29035918e-01,  3.60744202e-01,\n",
       "        5.96839712e-02,  3.24836533e-01,  1.51245783e-01,  3.12193837e-02,\n",
       "        2.16964250e-02,  2.48295790e-01,  2.79052935e-01,  3.11385019e-01,\n",
       "       -6.41323928e-02,  8.70509016e-02,  2.25618799e-01,  2.76500917e-01,\n",
       "        1.42647863e-01,  2.30605934e-01,  8.34613256e-02,  2.79037327e-01,\n",
       "        2.39486267e-01,  3.55706757e-01,  1.96060496e-01,  3.46020676e-01,\n",
       "        2.44675416e-01,  3.24004702e-02,  4.06771363e-01,  1.86164184e-01,\n",
       "        3.82991610e-01,  7.94954034e-02,  2.97438413e-01,  2.34343503e-01,\n",
       "        2.36665717e-01,  7.81791701e-02,  1.47370979e-01,  1.76142236e-01,\n",
       "        1.45275634e-01,  4.63329090e-01,  3.18972117e-01,  1.56475690e-01,\n",
       "        2.70381661e-01,  1.51110275e-01,  1.75224295e-01,  2.30915832e-01,\n",
       "        2.11224554e-01,  2.60176759e-01,  1.81242640e-01,  4.17469963e-02,\n",
       "        1.87205102e-01,  3.76241718e-01,  3.07014195e-01,  1.91932010e-01,\n",
       "        2.89361037e-01,  3.36329425e-01,  1.42442766e-01,  3.62953792e-01,\n",
       "        1.83192174e-01,  2.92618360e-01,  2.65187167e-01,  2.84071068e-01,\n",
       "        3.59612246e-01,  3.95469490e-01,  4.97974016e-01,  3.33400812e-01,\n",
       "        2.06444530e-01, -4.67414628e-02,  1.98164356e-01,  2.22735675e-01,\n",
       "        3.45759969e-01, -9.15202468e-02,  2.16337325e-01,  2.27102741e-01,\n",
       "        4.31387462e-01,  2.42942631e-01,  3.13929254e-01,  2.41990587e-01,\n",
       "        1.37271709e-01,  1.77687152e-01,  2.04218076e-01,  7.53119565e-02,\n",
       "        3.19671410e-01,  2.30535887e-01,  1.36405595e-01,  2.24467589e-01,\n",
       "        1.10348149e-01,  3.16329081e-01,  2.51499799e-01,  2.28173812e-01,\n",
       "        2.50983609e-01,  1.10450283e-01,  1.24133747e-01,  2.95232524e-01,\n",
       "        1.23960344e-01,  1.57298610e-01,  4.45458777e-02,  3.03475462e-01,\n",
       "        8.74593184e-02,  7.34630841e-02,  2.44739715e-01,  9.85120377e-02,\n",
       "        2.58940008e-01,  1.33461019e-01,  4.64821204e-02,  1.78414462e-01,\n",
       "        1.57348614e-01,  2.18776644e-01,  2.52816651e-01,  1.79669912e-01,\n",
       "        1.81912484e-01,  1.01144108e-01,  3.68554169e-01,  1.50703338e-01,\n",
       "        3.59436538e-01,  2.86438552e-01,  1.44770820e-01,  3.12512777e-01,\n",
       "        2.75787761e-01,  1.56778822e-01,  1.82531276e-01,  1.06424443e-01,\n",
       "        5.72050490e-02,  1.93086521e-01,  3.84759062e-01,  3.39237246e-01,\n",
       "        1.29793602e-01, -3.26974356e-03,  2.44024055e-01, -5.20683641e-02,\n",
       "        3.18994903e-01,  1.14332595e-01,  2.45958783e-01,  2.07551915e-01,\n",
       "        2.47542253e-01,  2.43300163e-01,  2.04056809e-01,  1.58703237e-01,\n",
       "        9.84507720e-02,  1.69805852e-01,  6.36116455e-02,  2.94913573e-01,\n",
       "        3.19234685e-01,  3.78916118e-01,  2.70413520e-01,  8.44571684e-02,\n",
       "        1.12473429e-01,  2.44606100e-01,  3.04234299e-01,  1.53890214e-01,\n",
       "        2.30722390e-01,  3.16223031e-01, -5.62473886e-02,  1.61041249e-02,\n",
       "        2.03773007e-01,  1.35381238e-01,  2.32381512e-01,  1.85003788e-01,\n",
       "        1.39565502e-01,  2.37569379e-01,  1.84614479e-01,  2.28957527e-01,\n",
       "        2.17696919e-01,  2.82692675e-01,  2.38826105e-01,  2.03324156e-01,\n",
       "        1.00465756e-01,  1.66387204e-01,  5.10015985e-01,  1.73529277e-01,\n",
       "        3.21236793e-01,  1.07548922e-01,  2.15180457e-01,  2.12357956e-01,\n",
       "        1.26836950e-01,  2.28695073e-01,  2.24086434e-02,  1.93622489e-01,\n",
       "        1.44161630e-01,  2.30301802e-01,  2.68318519e-01,  1.16302136e-01,\n",
       "        4.24799646e-01,  1.82328543e-01,  3.40356415e-01,  2.37615256e-01,\n",
       "        3.85345651e-01,  2.77650563e-01,  2.35832024e-01,  8.29455049e-02,\n",
       "        2.85667675e-01,  2.03621065e-01,  1.54608280e-01,  2.09076742e-01,\n",
       "        3.03934949e-01,  3.89647376e-01,  1.24456788e-01,  3.31858071e-01])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we could make our predictions like this:\n",
    "preds = X.dot(coef_) + intercept_\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our vector of predictions is 1000 rows, by 1 column\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69289092, 0.60135457],\n",
       "       [0.78841963, 0.33034955],\n",
       "       [0.42753444, 0.24694184],\n",
       "       [0.63211957, 0.81858974],\n",
       "       [0.75048724, 0.03439717],\n",
       "       [0.6276052 , 0.18382032],\n",
       "       [0.96199689, 0.35972355],\n",
       "       [0.89633581, 0.8763487 ],\n",
       "       [0.78989909, 0.38946304],\n",
       "       [0.97913371, 0.4550744 ]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, let's try a slightly different twist\n",
    "coef_ = np.random.random(20).reshape(10, 2)\n",
    "coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61194202,  0.37124985],\n",
       "       [ 0.33434484,  0.1353811 ],\n",
       "       [ 0.24539244,  0.26576138],\n",
       "       ...,\n",
       "       [ 0.5093821 ,  0.45236435],\n",
       "       [-0.10534285, -0.00812207],\n",
       "       [ 0.42219637,  0.17154141]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now run through the formula again\n",
    "preds = X.dot(coef_) + intercept_\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our predictions now have a second layer to them (Note -- these are NOT our actual predictions)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Keypoint:  For every single column we add to coef_, we'd have an additional column of output in our predictions\n",
    "\n",
    " - Assuming this represents our hidden layer, each additional column is referred to as a *neuron*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as an example, to add an additional neuron to our hidden layer\n",
    "coef_ = np.random.random(30).reshape(10, 3)\n",
    "preds = X.dot(coef_) + intercept_\n",
    "# this would be a hidden layer with 3 neurons\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question:  Is Our Data Currently A Linear or Non-Linear Representation of Our Input?\n",
    "\n",
    " - What if we just did this same process with another hidden layer?  Would anything change?\n",
    " - Can you derive non-linearity from a combination of strictly linear hidden units?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activation Functions\n",
    "\n",
    " - Used at each non-input layer to create a non-linear transformation from the previous step\n",
    " - Allow us to derive the non-linear patterns inherent within our data\n",
    " - Different activation functions are used at different points within a neural network:\n",
    "     - **hidden layers:** ReLu, others\n",
    "     - **output layer:** Sigmoid, Softmax, others -- used to make your final prediction\n",
    " - **Key Point**: activation functions in hidden units are meant to be 'gentle', and reduce non-linearity slowly, output activation functions are much stronger, used to assign a row to its most likely value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ReLu Activation Function\n",
    "\n",
    " - stands for Rectified Linear Unit\n",
    " - most commonly used activation function for hidden layers\n",
    "\n",
    "**Generic Function**:  \n",
    "\n",
    "$$ max(0, x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41837907, 0.38371288, 0.41460791],\n",
       "       [0.4172674 , 0.24030903, 0.27089342],\n",
       "       [0.19979873, 0.34171297, 0.1055203 ],\n",
       "       [0.14964119, 0.16027052, 0.01891691],\n",
       "       [0.37383954, 0.39087638, 0.27395295],\n",
       "       [0.08111006, 0.3363707 , 0.        ],\n",
       "       [0.44170424, 0.47014229, 0.33048733],\n",
       "       [0.10739369, 0.16921601, 0.08830669],\n",
       "       [0.21141957, 0.40442362, 0.21960638],\n",
       "       [0.22938957, 0.33537887, 0.35931258],\n",
       "       [0.11231355, 0.21665335, 0.24263517],\n",
       "       [0.23016502, 0.14222932, 0.26784108],\n",
       "       [0.50262578, 0.22022004, 0.41847515],\n",
       "       [0.15502927, 0.2050146 , 0.20231301],\n",
       "       [0.10919169, 0.21469923, 0.17217959],\n",
       "       [0.        , 0.0763882 , 0.03615379],\n",
       "       [0.23382187, 0.22899891, 0.24760317],\n",
       "       [0.18286833, 0.21654106, 0.15020049],\n",
       "       [0.28907719, 0.41671786, 0.33870905],\n",
       "       [0.17749718, 0.4794991 , 0.24994409]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_output = np.maximum(0, preds)\n",
    "hidden_output[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question Prompts:\n",
    "\n",
    " - Have we radically changed the shape of our data?\n",
    " - What's the benefit of this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Output Layer\n",
    "\n",
    " - Where you actually make your predictions\n",
    " - Is essentially the same output as `predict_proba` in SKlearn\n",
    " - Will have as many columns as unique categories that you're trying to predict\n",
    " - Is created essentially the same way as the hidden layer: randomly generate weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prompt:\n",
    "\n",
    "We want to predict a binary outcome.  What should the dimensions of our output layer be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "output_coef_ = np.random.normal(0, 0.1, 6).reshape(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.16436953, -0.10568609],\n",
       "       [ 0.05983523, -0.07302869],\n",
       "       [-0.01900292,  0.21000109]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# we'll now use this to create our final predictions -- for this round\n",
    "final_output = hidden_output.dot(output_coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we now have a 1000 x 2 array with our output\n",
    "final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05368798,  0.01482922],\n",
       "       [-0.05935487, -0.0047609 ],\n",
       "       [-0.01439954, -0.02391142],\n",
       "       [-0.0153661 , -0.02354677],\n",
       "       [-0.04326556, -0.01052441],\n",
       "       [ 0.0067948 , -0.03313692],\n",
       "       [-0.05075187, -0.01161317],\n",
       "       [-0.00920526, -0.00516314],\n",
       "       [-0.01472532, -0.00576105],\n",
       "       [-0.02446517,  0.02672047]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it gives us this\n",
    "final_output[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48658123, 0.50370724],\n",
       "       [0.48516564, 0.49880978],\n",
       "       [0.49640018, 0.49402243],\n",
       "       [0.49615855, 0.49411358],\n",
       "       [0.4891853 , 0.49736892],\n",
       "       [0.50169869, 0.49171653],\n",
       "       [0.48731476, 0.49709674],\n",
       "       [0.4976987 , 0.49870922],\n",
       "       [0.49631874, 0.49855974],\n",
       "       [0.49388401, 0.50667972]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now we provide our final activation function -- the sigmoid!\n",
    "from scipy.special import expit\n",
    "# expit is another name for the sigmoid function\n",
    "predict_proba = expit(final_output)\n",
    "predict_proba[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# and get our final predictions\n",
    "final_predictions = np.argmax(predict_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 1, 0, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some Important Notes:  \n",
    "\n",
    " - What we just did is called *forward propagation*\n",
    " - This is basically how you churn through layers of computation to get predictions in a neural network\n",
    " - You go through this process multiple times when fitting a neural network, iteratively updating the weights after each round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lab Wrap Up:\n",
    "\n",
    " - forward propagation is the basic machinery of how a neural network makes its predictions\n",
    " - the presence of hidden layers + non-linear activation functions allows a neural network to go beyond linear models and tease out hidden patterns in raw data\n",
    " - when training a neural network, you go through many round (typically 10-50) of forward propagation to converge on the appropriate weights for each variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Additional Questions:\n",
    "\n",
    " - how many hidden layers do you add? how many neurons?\n",
    " - how exactly are the weights updated?  \n",
    " - how does one implement neural networks in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### On Hidden Layers.......\n",
    "\n",
    " - It has been shown that any problem that can be approximated with a neural network can be done *with just one hidden layer*......although this is not always the fastest way to do things.\n",
    " - In practice it's usually best to keep the number of hidden layers to a minimum, to avoid overfitting\n",
    " - When a neural network has 2 or more hidden layers this is typically when you start Deep Learning.\n",
    " - A lot of research has been done on different types of layers to use in a neural network, which has expanded the amount of hidden layers that can feasibly be added to a neural network for various tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep Learning Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " - Two major ones are Tensorflow & PyTorch\n",
    " - Developed by Google & Facebook, respectively\n",
    " - Tensorflow tends to be used more for production, PyTorch for prototyping & development\n",
    " - Tensorflow has a high-level API called Keras, which is very easy to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Keras\n",
    "\n",
    " - wrapper built around Tensorflow to make it easy to construct neural networks\n",
    " - essentially a connector set for Neural Network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(train_img, train_label), (test_img, test_label) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# standardize your data\n",
    "train_img = train_img / 255.0\n",
    "\n",
    "test_img = test_img / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# this is the equivalent of what we just created in the previous lab\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(5, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compiling your model\n",
    "\n",
    " - optimizer: strategy you use to update your weights\n",
    " - loss: loss function you use to optimize when training\n",
    "     - 'sparse_categorical_crossentropy': encoding your labels as 0, 1, 2, 3, 4, etc (most common)\n",
    " - metrics: metric you use to score your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Optimizers:\n",
    "\n",
    " - Technique you use for updating your weights\n",
    " - All built around the same basic concept:  gradient descent\n",
    " - Common choices:\n",
    "  - **sgd**: stochastic gradient descent (most common)\n",
    "  - **adam**: updated version of sgd, formulated in 2015.....updates itself as training moves on\n",
    " - Most common parameter to play around with:  **the learning rate**\n",
    " - Typical values will be anywhere from 0.0001 to 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Learning Rate:  The Speed With Which You Increase Your Weight Size\n",
    "\n",
    " - Updating weights in a neural network is based off of the derivative of your cost function \n",
    " - The learning rate is the size of the 'step' that you take in the direction of the derivative\n",
    " - Essentially:\n",
    "  - a larger learning rate will converge faster, but potentially 'skip' over more ideal versions of your weights\n",
    "  - a smaller one will have the opposite problem, and potentially get stuck in local minima\n",
    " - Essentially a way of handling the bias-variance problem in Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# you would change the parameters of your optimizer in the following way\n",
    "sgd = keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's fit our model - epochs is the rounds of forward propagation\n",
    "model.fit(train_img, train_label, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# and finally - score our model\n",
    "test_loss, test_acc = model.evaluate(test_img, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lab:  The Fashion MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load in the dataset this way\n",
    "mnist = keras.datasets.fashion_mnist\n",
    "(train_img, train_label), (test_img, test_label) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1:  \n",
    " - Go ahead and build your model using the *same* parameters that we had before:\n",
    "  - Flattened input layer\n",
    "  - Hidden layers with 5 & 10 neurons + ReLu activation function\n",
    "  - Output layer with 10 neurons + sigmoid activation\n",
    "  \n",
    "What are our results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 2:  Can you get to around ~ 90% accuracy?\n",
    "\n",
    "Try getting better results by trying the following parameters:\n",
    "\n",
    " - The number of neurons in your hidden layers\n",
    " - Switching the final activation function from sigmoid to softmax\n",
    " - Increasing or decreasing your learning rate\n",
    " - Changing the number of epochs\n",
    " \n",
    "Take 20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Basic Rules For Hidden Layers/Neurons:\n",
    "\n",
    " - If the sole purpose of a hidden layer is for non-linear computation, usually no more than 2 is needed\n",
    " - 1 is often sufficient\n",
    " - Adding neurons is typically better for teasing out non-linear boundaries for one task or another\n",
    " - number of neurons:\n",
    "  - Best to add somewhere between a total number between the number of columns in your input and output layer\n",
    "  - Can typically adjust depending on whether or not your model is overfitting or underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Basic Rules for Training Neural Networks\n",
    "\n",
    " - small learning rate + lots of hidden units is safest best for working with complicated data\n",
    " - you can also use regularization to curb overfitting\n",
    " - you don't use traditional cross validation.  You use drop out instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Layers You Can Add to A Neural Network:\n",
    "\n",
    " - **Dense**:  Each input unit is connected to every single output unit.  Ie, a linear combination.  This is what we've been doing so far.  These are the most straightforward.\n",
    " - **Convolutional**:  Best for processing images.  Segments your data into smaller chunks when forming connections.\n",
    " - **Recurrent**:   Maintain information about previous input from last round of backpropagation.  Therefore have a notion of sequence.  Useful for items where the order of items matters.  Language processing especially."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
